{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import flash\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import torch\n",
    "import torchvision\n",
    "from ipynb.fs.defs.dcgan import fix_seed, init_weight\n",
    "from PIL import Image\n",
    "from sklearn.datasets import fetch_openml\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2913\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention GAN\n",
    "\n",
    "Self-Attention GAN(SAGAN)は、`Self-Attention`, `Pointwise Convolution`, `Spectral Normalization`の3つの技術を軸に構成される。これらの内容は難しいため、書籍からさらに説明を深めて具体例を用いながら説明してく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "GANの`ConvTranspose2d`の課題は、入力データの局所的な情報しか使われないことであった。\n",
    "\n",
    "例えば以下のように`kernel_size=2`の場合には、入力の各ピクセルは、出力の2x2のピクセルにしか影響を与えない。逆に、出力の各ピクセルはせいぜい2つの入力ピクセルからしか影響を受けない。\n",
    "\n",
    "したがって、入力の全体情報が使われていない。この問題に対処するためにSelf-Attentionが応用される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 2, 2)\n",
    "print(\"--------x--------\")\n",
    "display(x)\n",
    "t = torch.nn.ConvTranspose2d(1, 1, kernel_size=2)\n",
    "print(\"--------weight--------\")\n",
    "display(t.weight)\n",
    "print(\"--------bias--------\")\n",
    "display(t.bias)\n",
    "print(\"--------output--------\")\n",
    "display(t(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Attentionでは、あるレイヤーに入力する前に、画像を以下のように変換する。\n",
    "\n",
    "$\n",
    "y = x + \\gamma o\n",
    "$\n",
    "\n",
    "xは入力画像、$\\gamma$は係数(学習時に決定される)、$o$はSelf-Attention Mapである。\n",
    "\n",
    "Self-Attention Mapには、「大域的な」情報が含まれている。ここが少しわかりづらいので、以下、具体的な計算例を見ていきながら、Self-Attention Mapについて理解することにする。\n",
    "\n",
    "*Todo:xとoを足すと色々と情報が失われる気がする。なぜ別々に入力しないのか？*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, gray=False, scale=2):\n",
    "    # set figsize\n",
    "    plt.figure(figsize=(scale * img.shape[-1], scale * img.shape[-2]))\n",
    "    channel = img.shape[1]\n",
    "    if gray:\n",
    "        plt.gray()\n",
    "        plt.imshow(img.squeeze(0))\n",
    "    else:\n",
    "        plt.imshow(img.squeeze(0).permute(1, 2, 0))\n",
    "\n",
    "    if not gray:\n",
    "        for i in range(channel):\n",
    "            for j in range(img.shape[2]):\n",
    "                for k in range(img.shape[3]):\n",
    "                    color = (\n",
    "                        \"black\" if torch.mean(img[0, :, j, k]).item() > 0.5 else \"white\"\n",
    "                    )\n",
    "                    plt.text(\n",
    "                        k,\n",
    "                        j + 0.2 * (i - 1),\n",
    "                        \"{:.2f}\".format(img[0, i, j, k].item()),\n",
    "                        ha=\"center\",\n",
    "                        va=\"center\",\n",
    "                        fontsize=8 * scale,\n",
    "                        color=color,\n",
    "                    )\n",
    "    if gray:\n",
    "        for j in range(img.shape[1]):\n",
    "            for k in range(img.shape[2]):\n",
    "                color = (\n",
    "                    \"black\"\n",
    "                    if (img[0, j, k].item() - torch.min(img).item())\n",
    "                    / (torch.max(img).item() - torch.min(img).item())\n",
    "                    > 0.5\n",
    "                    else \"white\"\n",
    "                )\n",
    "                plt.text(\n",
    "                    k,\n",
    "                    j,\n",
    "                    \"{:.2f}\".format(img[0, j, k].item()),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=8 * scale,\n",
    "                    color=color,\n",
    "                )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の例では、入力xは(1,3,4,4)のサイズのテンソルであるとする。dim=0はバッチサイズ、dim=1はRGBのチャネルの次元、dim=2,3は画像の高さ・幅の次元となっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 4\n",
    "# C x H x W\n",
    "x = torch.rand(1, 3, img_size, img_size)\n",
    "display(x.shape)\n",
    "display(x)\n",
    "show_image(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、チャネルの次元を残しつつ、高さ・幅の次元を1つにすることで、サイズが(1,3,16)のテンソルx_flattenを作成する。\n",
    "\n",
    "`x_flatten[0,:,i]`は、各indexのRGBのチャネル情報となる。\n",
    "\n",
    "ここで、以降の説明をわかりやすくするため、`x_flatten`でフラット化された各ピクセルのチャネル情報を以下のように表すとする。\n",
    "\n",
    "$\n",
    "x\\_flatten_i = \\begin{pmatrix} r_i \\\\ g_i \\\\ b_i \\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flatten = x.reshape(1, x.shape[1], -1)\n",
    "print(\"--------x_flatten--------\")\n",
    "display(x_flatten.shape)\n",
    "display(x_flatten)\n",
    "show_image(x_flatten.unsqueeze(2))\n",
    "show_image(x_flatten, gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_flatten`の転置行列`x_flatten_t`を作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flatten_t = x_flatten.transpose(1, 2)\n",
    "print(\"--------x_flatten_t--------\")\n",
    "display(x_flatten_t.shape)\n",
    "display(x_flatten_t)\n",
    "show_image(x_flatten_t, gray=True, scale=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_flatten_t`と`x_flatten`の積をとって、`s`を作成する。この`s`は[グラム行列](https://ja.wikipedia.org/wiki/%E3%82%B0%E3%83%A9%E3%83%A0%E8%A1%8C%E5%88%97)と呼ばれる。グラム行列は、以下のような性質を持つ。\n",
    "\n",
    "- 正方行列(ここでのサイズは`(16,16)`)\n",
    "- 正則行列\n",
    "- 対称行列\n",
    "- $s_{i,j}$は、$x\\_{flatten}_i \\cdot x\\_{flatten}_j$となる。すなわち、`s`の各成分は、元の画像のピクセルのチャネル情報ベクトル同士に対して内積をとったものになっている。\n",
    "- なお、チャネル情報の内積を取っているのだから、内積の計算に使用した2つのチャネルベクトルの大きさで内積値を割ることにより、$\\cos{\\theta_{i,j}} = x\\_{flatten}_i \\cdot x\\_{flatten}_j / (|x\\_{flatten}_i|\\cdot|x\\_{flatten}_j|)$となるから、2つのチャネル情報ベクトルの成す角が計算される。これは相関係数を計算していることに他ならない。\n",
    "- 上記より、`s`には各チャネル情報の類似度のような情報が含まれていることがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.bmm(x_flatten_t, x_flatten)\n",
    "display(s.shape)\n",
    "display(s)\n",
    "show_image(s, gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本当に`s`の各成分がチャネル情報の内積になっているのかを確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "for i in range(img_size * img_size):\n",
    "    for j in range(img_size * img_size):\n",
    "        diff = abs(\n",
    "            torch.dot(x_flatten[0, :, i], x_flatten[0, :, j]).item() - s[0, i, j].item()\n",
    "        )\n",
    "        assert diff < eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行方向に対してSoftmax関数による正規化を行う。これは、Attention Mapを転置したものになる(まだ`o`(**Self-**Attention Mapではないので注意)。\n",
    "\n",
    "*Todo:この正規化は必須？*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=-2)\n",
    "attention_map_t = m(s)\n",
    "print(\"--------attention_map_t--------\")\n",
    "display(attention_map_t.shape)\n",
    "# display(attention_map_t)\n",
    "# show_image(attention_map_t, gray=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_map = attention_map_t.transpose(1, 2)\n",
    "print(\"--------attention_map--------\")\n",
    "display(attention_map.shape)\n",
    "display(attention_map)\n",
    "show_image(attention_map, gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、x_flattenとAttention Mapの積を取る。これにより、各ピクセルは、そのピクセルと同じような色が多ければ大きくなる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.bmm(x_flatten, attention_map_t)\n",
    "# make the size of o same as x\n",
    "o = o.reshape(1, x.shape[1], x.shape[2], x.shape[3])\n",
    "print(\"--------o--------\")\n",
    "display(o.shape)\n",
    "display(o)\n",
    "show_image(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでの処理をクラスとして実装する。書籍とは変えて、SelfAttentionクラスにはSelfAttentionの実装のみを書く(Pointwise Convは別で書く)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimitiveSelfAttention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = torch.nn.Softmax(dim=-2)\n",
    "        self.gamma = torch.nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flatten = x.view(x.shape[0], x.shape[1], -1)\n",
    "        x_flatten_t = x_flatten.permute(0, 2, 1)\n",
    "        s = torch.bmm(x_flatten_t, x_flatten)\n",
    "        attention_map_t = self.softmax(s)\n",
    "        attention_map = attention_map_t.permute(0, 2, 1)\n",
    "        o = torch.bmm(x_flatten, attention_map_t).view(\n",
    "            x.shape[0], x.shape[1], x.shape[2], x.shape[3]\n",
    "        )\n",
    "        return x + self.gamma * o, attention_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lenaの画像に対して、Attention Mapを確認してみる。\n",
    "\n",
    "Lenaの画像テンソルのサイズは、(3,64,64)にしている。したがって、Attention Mapのサイズは、$(64*64, 64*64)=(4096,4096)$になる。Attention Mapのサイズはかなり大きくなり、入力画像のサイズが大きすぎる時にはメモリに乗らなくなるので注意。\n",
    "\n",
    "下記の実装で示している通り、`attention_map[0,:,x*img_size+y]`の1次元ベクトルを2次元ベクトルに変換すると、(x,y)に対応したAttentionを見るおがことができる。下図の可視化では、ランダムに選ばれた\"X\"のマーカーに対するAttentionをヒートマップで表示している。オリジナルの画像と見比べると、マーカーの位置と同じような色の部分が濃く(より赤色に)表示されていることがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download lena from https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png\n",
    "if not os.path.exists(\"data/lena.png\"):\n",
    "    url = \"https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png\"\n",
    "    response = requests.get(url)\n",
    "    with open(\"data/lena.png\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "img_size = 64\n",
    "# open lena and convert it to tensor\n",
    "lena = Image.open(\"data/lena.png\")\n",
    "lena = torchvision.transforms.Resize((img_size, img_size))(lena)\n",
    "lena = torchvision.transforms.ToTensor()(lena).to(device)\n",
    "self_attention = PrimitiveSelfAttention().to(device)\n",
    "_, attention_map = self_attention(lena.unsqueeze(0))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.imshow(lena.detach().cpu().permute(1, 2, 0))\n",
    "ax.set_title(\"original\")\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "fig.suptitle(\"attention map\")\n",
    "for ax in axes.flatten():\n",
    "    # random sample from0~img_size\n",
    "    x = random.randint(0, img_size - 1)\n",
    "    y = random.randint(0, img_size - 1)\n",
    "    # ax.imshow(attention_map[0, :, x*img_size+y].detach().cpu().reshape(img_size, img_size))\n",
    "    sns.heatmap(\n",
    "        attention_map[0, :, x * img_size + y]\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .reshape(img_size, img_size),\n",
    "        ax=ax,\n",
    "        cbar=False,\n",
    "        cmap=\"jet\",\n",
    "    )\n",
    "    # highlight x,y\n",
    "    ax.scatter(y, x, c=\"black\", s=100, marker=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lena, attention_map, self_attention\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise Convolution\n",
    "\n",
    "Pointwise Convolutionはkernel_sizeが1のあた畳み込み。画像の場合であれば、高さ・幅の次元は変化させないままに、チャネルの次元を変えることができる。kernel_size=1なので、周辺情報を含めた特徴量の作成はできないものの、チャネルの出力次元を入力次元を小さくすることにより、次元の圧縮を行うことができる。\n",
    "\n",
    "先ほどのLenaの画像を用いたSelf Attentionでも説明した通り、Attention Mapのサイズはかなり大きくなる。そこで、Pointwise Convolutionによる次元圧縮を行うことにより、Attention Mapのサイズを小さくしつつ、Self-Attention Layerを適用できることがポイントになる。\n",
    "\n",
    "Self-Attentionでは、`key`, `query`, `value`という用語が一般的らしい。基本的な操作は上記と同じであるものの、`key`, `query`, `value`のそれぞれに対してPointwise Convolutionを行うことがポイント。詳細は下図。\n",
    "\n",
    "```mermaid\n",
    "flowchart LR;\n",
    "\n",
    "input --[Pointwise Conv]--> key ----> dot1([x])\n",
    "input --[Pointwise Conv]--> query --[transpose]-->query_t ----> dot1 --[Softmax]--> attention_map_t ----> dot2([x]) \n",
    "input --[Pointwise Conv]--> value ----> dot2 ----> o\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, in_channels, feat_channels):\n",
    "        super().__init__()\n",
    "        self.softmax = torch.nn.Softmax(dim=-2)\n",
    "        self.gamma = torch.nn.Parameter(torch.zeros(1))\n",
    "        self.convs = torch.nn.ModuleDict(\n",
    "            {\n",
    "                key: torch.nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=feat_channels if key != \"value\" else in_channels,\n",
    "                    kernel_size=1,\n",
    "                )\n",
    "                for key in [\"key\", \"query\", \"value\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, height, width = x.shape\n",
    "        key = self.convs[\"key\"](x).view(batch_size, -1, height * width)\n",
    "        query = (\n",
    "            self.convs[\"query\"](x).view(batch_size, -1, height * width).permute(0, 2, 1)\n",
    "        )\n",
    "        s = torch.bmm(query, key)\n",
    "\n",
    "        attention_map_t = self.softmax(s)\n",
    "        attention_map = attention_map_t.permute(0, 2, 1)\n",
    "\n",
    "        value = self.convs[\"value\"](x).view(batch_size, -1, height * width)\n",
    "        o = torch.bmm(value, attention_map_t).view(batch_size, -1, height, width)\n",
    "\n",
    "        return x + self.gamma * o, o, attention_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch_size = 4\n",
    "_input = torch.rand(_batch_size, 3, 64, 64)\n",
    "_self_attention = SelfAttention(3, 1)\n",
    "_output, _o, _attention_map = _self_attention(_input)\n",
    "assert _output.shape == _input.shape\n",
    "assert _o.shape == _input.shape\n",
    "assert _attention_map.shape == (_batch_size, 64 * 64, 64 * 64)\n",
    "del _input, _output, _o, _attention_map, _self_attention\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Normalization\n",
    "\n",
    "`wgan.ipynb`では、Discriminatorのリプシッツ連続性が重要であると述べた。まず、リプシッツ連続性について理解する。\n",
    "\n",
    "ある関数$f(x)$がリプシッツ連続であるとは、任意の$x_1, x_2$に対して、$|f(x_1) - f(x_2)| \\leq K |x_1 - x_2|$が成立するリプシッツ定数$K$が存在することをいう。これを定性的にとらえると、入力の変化に対して出力の変化は上限があるということになる。\n",
    "\n",
    "具体例で考えると、$f(x)=x^2$はリプシッツ連続ではないが、$f(x)=\\sqrt(x)$はリプシッツ連続である。$x^2$は$x$が大きくなるにつれて急激に変化するものの、$\\sqrt(x)$はそうではないので直感的に正しそうなことがわかると思う。なお、証明も難しくない。\n",
    "\n",
    "Discriminator, Generatorがリプシッツ連続であれば、入力が多少変化したとしても出力が大きく変化することはない。そこで、これらをリプシッツ連続にするために、Spectral NormalizationをGenerator, Discriminatorの重みに適用する。これにより、リプシッツ定数を1以下に制限することができる。\n",
    "\n",
    "Pytorchでは、`torch.nn.utils.spectral_norm`を使用することで実装が簡単になる。\n",
    "\n",
    "以下の実装では、WGANと同様にWasserstein Lossを使用するので、出力は書籍と異なる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deconv2d(torch.nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, with_batch_norm=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.add_module(\n",
    "            \"ConvTranspose2d\",\n",
    "            torch.nn.utils.spectral_norm(\n",
    "                torch.nn.ConvTranspose2d(in_channels, out_channels, **kwargs)\n",
    "            ),\n",
    "        )\n",
    "        if with_batch_norm:\n",
    "            self.add_module(\"BatchNorm2d\", torch.nn.BatchNorm2d(out_channels))\n",
    "        self.add_module(\"ReLU\", torch.nn.ReLU(inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Sequential):\n",
    "    def __init__(self, z_dim, image_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                Deconv2d(z_dim, image_size * 8, kernel_size=4, stride=1),\n",
    "                Deconv2d(\n",
    "                    image_size * 8, image_size * 4, kernel_size=4, stride=2, padding=1\n",
    "                ),\n",
    "                Deconv2d(\n",
    "                    image_size * 4, image_size * 2, kernel_size=4, stride=2, padding=1\n",
    "                ),\n",
    "                SelfAttention(image_size * 2, image_size * 2 // 8),\n",
    "                Deconv2d(\n",
    "                    image_size * 2, image_size, kernel_size=4, stride=2, padding=1\n",
    "                ),\n",
    "                SelfAttention(image_size, image_size // 8),\n",
    "                Deconv2d(\n",
    "                    image_size,\n",
    "                    1,\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    with_batch_norm=False,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        supplement = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, SelfAttention):\n",
    "                x, o, attention_map = layer(x)\n",
    "                supplement[f\"o_{i+1}\"] = o.detach().cpu()\n",
    "                supplement[f\"attention_map_{i+1}\"] = attention_map.detach().cpu()\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        if self.training:\n",
    "            return x\n",
    "        return x, supplement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch_size = 4\n",
    "_input = torch.rand(_batch_size, 20, 1, 1)\n",
    "_generator = Generator(20, 64)\n",
    "_output = _generator(_input)\n",
    "assert _output.shape == (_batch_size, 1, 64, 64)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.utils.spectral_norm(\n",
    "                    torch.nn.Conv2d(1, image_size, kernel_size=4, stride=2, padding=1)\n",
    "                ),\n",
    "                torch.nn.LeakyReLU(0.1, inplace=True),\n",
    "                torch.nn.utils.spectral_norm(\n",
    "                    torch.nn.Conv2d(\n",
    "                        image_size, image_size * 2, kernel_size=4, stride=2, padding=1\n",
    "                    )\n",
    "                ),\n",
    "                torch.nn.LeakyReLU(0.1, inplace=True),\n",
    "                torch.nn.utils.spectral_norm(\n",
    "                    torch.nn.Conv2d(\n",
    "                        image_size * 2,\n",
    "                        image_size * 4,\n",
    "                        kernel_size=4,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                    )\n",
    "                ),\n",
    "                torch.nn.LeakyReLU(0.1, inplace=True),\n",
    "                SelfAttention(image_size * 4, image_size * 4 // 8),\n",
    "                torch.nn.utils.spectral_norm(\n",
    "                    torch.nn.Conv2d(\n",
    "                        image_size * 4,\n",
    "                        image_size * 8,\n",
    "                        kernel_size=4,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                    )\n",
    "                ),\n",
    "                torch.nn.LeakyReLU(0.1, inplace=True),\n",
    "                SelfAttention(image_size * 8, image_size * 8 // 8),\n",
    "                torch.nn.utils.spectral_norm(\n",
    "                    torch.nn.Conv2d(\n",
    "                        image_size * 8, 1, kernel_size=4, stride=1, padding=0\n",
    "                    )\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        supplement = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, SelfAttention):\n",
    "                x, o, attention_map = layer(x)\n",
    "                supplement[f\"o_{i+1}\"] = o.detach().cpu()\n",
    "                supplement[f\"attention_map_{i+1}\"] = attention_map.detach().cpu()\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        if self.training:\n",
    "            return x.view(-1)\n",
    "        return x.view(-1), supplement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch_size = 4\n",
    "_input = torch.rand(_batch_size, 1, 64, 64)\n",
    "_discriminator = Discriminator(64)\n",
    "_output = _discriminator(_input)\n",
    "assert _output.shape == (_batch_size,)\n",
    "del _input, _output, _discriminator\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "\n",
    "Self-Attention層でメモリの使用量が多くなるため、バッチサイズをあまり大きくできないことに注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistTransform:\n",
    "    interpolation_modes = [\n",
    "        torchvision.transforms.InterpolationMode.NEAREST,\n",
    "        torchvision.transforms.InterpolationMode.NEAREST_EXACT,\n",
    "        torchvision.transforms.InterpolationMode.BILINEAR,\n",
    "        torchvision.transforms.InterpolationMode.BICUBIC,\n",
    "    ]\n",
    "\n",
    "    def __init__(self, image_size):\n",
    "        self.transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.RandomChoice(\n",
    "                    [\n",
    "                        torchvision.transforms.Resize(\n",
    "                            (image_size, image_size), interpolation=mode\n",
    "                        )\n",
    "                        for mode in self.interpolation_modes\n",
    "                    ]\n",
    "                ),\n",
    "                torchvision.transforms.RandomRotation(10),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                # add random noize, std=0.1\n",
    "                torchvision.transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "transform = MnistTransform(image_size)\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, transform=transform, target_transform=None, download=True\n",
    ")\n",
    "val_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    target_transform=None,\n",
    "    download=True,\n",
    ")\n",
    "dataset = train_dataset + val_dataset\n",
    "batch_size = 512\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count(),\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 20\n",
    "num_epochs = 100\n",
    "n_critics = 1\n",
    "\n",
    "num_image_to_save = 8 * 4\n",
    "z_fixed = torch.randn(num_image_to_save, z_dim, 1, 1, device=device)\n",
    "\n",
    "generator = Generator(z_dim, image_size).to(device)\n",
    "discriminator = Discriminator(image_size).to(device)\n",
    "generator.apply(init_weight)\n",
    "discriminator.apply(init_weight)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "lr_d, betas_d = 0.0001, (0.5, 0.999)\n",
    "lr_g, betas_g = 0.0001, (0.5, 0.999)\n",
    "optimizer_d = flash.core.optimizers.LAMB(\n",
    "    discriminator.parameters(), lr=lr_d, betas=betas_d\n",
    ")\n",
    "optimizer_g = flash.core.optimizers.LAMB(generator.parameters(), lr=lr_g, betas=betas_g)\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "datetime_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_dir = f\"./sagan/{datetime_str}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir=os.path.join(save_dir, \"logs\"))\n",
    "writer.add_text(\"Config/discriminator\", f\"lr_d: {lr_d}, betas_d: {betas_d}\")\n",
    "writer.add_text(\"Config/generator\", f\"lr_g: {lr_g}, betas_g: {betas_g}\")\n",
    "writer.add_text(\n",
    "    \"Config/Common\",\n",
    "    f\"z_dim: {z_dim}, n_critics: {n_critics}, batch_size: {batch_size}, num_epochs: {num_epochs}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "print(\"Start training\")\n",
    "for epoch in range(num_epochs):\n",
    "    start.record()\n",
    "    minibatch_d_losses = []\n",
    "    minibatch_g_losses = []\n",
    "\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    for i, (images, _) in enumerate(train_data_loader):\n",
    "        mini_batch = images.size()[0]\n",
    "        images = images.to(device)\n",
    "        if mini_batch == 1:\n",
    "            continue\n",
    "\n",
    "        for _ in range(n_critics):\n",
    "            # train discriminator\n",
    "            discriminator.zero_grad()\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                z = torch.randn(mini_batch, z_dim, 1, 1, device=device)\n",
    "\n",
    "                fake_images = generator(z)\n",
    "                loss_d_real = discriminator(images).mean()\n",
    "                loss_d_fake = discriminator(fake_images.detach()).mean()\n",
    "                loss_d = -loss_d_real + loss_d_fake\n",
    "            scaler.scale(loss_d).backward()\n",
    "            scaler.step(optimizer_d)\n",
    "            scaler.update()\n",
    "            minibatch_d_losses.append(loss_d.item())\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "        # train generator\n",
    "        generator.zero_grad()\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            loss_g = -discriminator(fake_images).mean()\n",
    "        scaler.scale(loss_g).backward()\n",
    "        scaler.step(optimizer_g)\n",
    "        scaler.update()\n",
    "        minibatch_g_losses.append(loss_g.item())\n",
    "\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\n",
    "        f\"Epoch: {epoch+1}/{num_epochs}, d_loss: {np.mean(minibatch_d_losses):.4f}, g_loss: {np.mean(minibatch_g_losses):.4f}, time: {start.elapsed_time(end)/1000:.4f} sec\"\n",
    "    )\n",
    "\n",
    "    writer.add_scalars(\n",
    "        \"Loss\",\n",
    "        {\n",
    "            \"discriminator\": np.mean(minibatch_d_losses),\n",
    "            \"generator\": np.mean(minibatch_g_losses),\n",
    "        },\n",
    "        epoch,\n",
    "    )\n",
    "    writer.add_scalar(\"Time\", start.elapsed_time(end) / 1000, epoch)\n",
    "    generator.eval()\n",
    "    fake_images, supplement = generator(z_fixed)\n",
    "    writer.add_images(\n",
    "        \"Images/generator\",\n",
    "        fake_images.detach().cpu(),\n",
    "        epoch,\n",
    "    )\n",
    "    for key in [\"attention_map_4\", \"attention_map_6\"]:\n",
    "        am_size = int(np.prod(supplement[key].shape[1:], axis=0) ** 0.25)\n",
    "        ams = supplement[key][0].squeeze().view([am_size for _ in range(4)])\n",
    "        # choose 8 from 0~am_size uniformly\n",
    "        xs = np.linspace(0, am_size - 1, 8).astype(int)\n",
    "        ys = np.linspace(0, am_size - 1, 8).astype(int)\n",
    "        writer.add_image(\n",
    "            f\"Images/generator_{key}\",\n",
    "            ams[xs, ys, :, :].unsqueeze(1),\n",
    "            epoch,\n",
    "            dataformats=\"NCHW\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習結果の確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "- [つくりながら学ぶ！PyTorchによる発展ディープラーニング | 小川 雄太郎 | 工学 | Kindleストア | Amazon](https://www.amazon.co.jp/%E3%81%A4%E3%81%8F%E3%82%8A%E3%81%AA%E3%81%8C%E3%82%89%E5%AD%A6%E3%81%B6%EF%BC%81PyTorch%E3%81%AB%E3%82%88%E3%82%8B%E7%99%BA%E5%B1%95%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0-%E5%B0%8F%E5%B7%9D-%E9%9B%84%E5%A4%AA%E9%83%8E-ebook/dp/B07VPDVNKW/ref=sr_1_1?__mk_ja_JP=%E3%82%AB%E3%82%BF%E3%82%AB%E3%83%8A&crid=39VBRPTDUUH0F&keywords=%E4%BD%9C%E3%82%8A%E3%81%AA%E3%81%8C%E3%82%89%E5%AD%A6%E3%81%B6+pytorch&qid=1701503265&sprefix=%E4%BD%9C%E3%82%8A%E3%81%AA%E3%81%8C%E3%82%89%E5%AD%A6%E3%81%B6+pytorch%2Caps%2C221&sr=8-1)\n",
    "- [[1802.05957] Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
